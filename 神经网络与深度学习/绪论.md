## 机器学习

机器学习常常分为以下几个步骤

- 数据预处理
- 特征提取
- 特征转换
  - 降维
    - 特征抽取
    - 特征选择
    - 主成分分析（PCA）
    - 线性判别分析（LDA）
  - 升维

- 预测

## 表示学习

如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作表 示学习（Representation Learning）

### 局部表示

用一个$|V|$维的$one-hot$​向量来表示每一种颜色，在第$i$种颜色对应的$one-hot$向量中，第$i$维的值为$1$，其他都为$0$。

### 分布式表示

使用RGB值来表示颜色，不同颜色对应到$R、G、B$三维空间中的一个点，同时表示为低维的稠密向量

三维的稠密向量可以表示所有的颜色，分布式表示也容易表示新的颜色名，不同颜色之间的相似度也很容易计算

嵌入指的是将一个度量空间中的一些对象映射到另一个低维的度量空间中，尽可能保持不同对象间的拓扑关系

在低维的嵌入空间中每个样本都不在坐标轴上，样本之间可以计算相似度



要学习到一种好的高层语义表示，通常需要从底层特征开始，经过多步非线性转换才能得到

表示学习的关键在于构建具有一定深度的多层次特征表示

## 深度学习

和浅层学习不同的是，深度学习需要解决的关键问题是贡献度分配问题，即一个系统中不同的组件或其参数对最终系统输出结果的贡献或影响。

目前，深度学习采用的模型主要是神经网络模型，主要原因是神经网络模型可以使用误差反向传播算法，从而可以比较好的解决贡献度分配问题

## 神经网络

机器学习领域，神经网络是由很多人工神经元构成的网络结构模型

### 人脑神经网络

- 细胞体

- 细胞突起
  - 树突：接收刺激并将兴奋传入细胞体，每个神经元有一个或多个树突
  - 轴突：将自身的兴奋状态从胞体传送到另一个神经单元或其他组织，每个神经元只有一个轴突

- 当神经元 A 的一个轴突和神经元 B 很近，足以 对它产生影响，并且持续地、重复地参与了对神经元 B 的兴奋，那么在这两个神 经元或其中之一会发生某种生长过程或新陈代谢变化，以致神经元 A 作为能使 神经元 B 兴奋的细胞之一，它的效能加强了，这个机制称为赫布理论
- 如果两个神经元总是相关联地受到刺激，它们之间的突触强度增加，这样的学习方法称为赫布型学习
- 分为长期记忆和短期记忆
- 短期记忆转换为长期记忆的过程称为凝固作用
- 人脑中的海马区为大脑结构凝固作用的核心区域

### 人工神经网络

由多个节点互相连接而成，可以用来对数据之间的复杂关系进行建模，不同节点之间的连接被赋予不同的权重，每个权重代表一个节点对另一个节点的影响大小，每个节点都代表一种特定函数

- 感知器是最早的具有机器学习思想的神经网络
- 直到1980年左右，反向传播算法才有效解决多层神经网络的学习问题

#### 发展历史

- 模型提出
  - 第一个高潮期

- 冰河期
  - 第一个低谷期

- 反向传播算法引起的复兴

- 流行度降低
- 深度学习的崛起

## 常用的深度学习框架

- Theano
- Caffe 主要用于计算机视觉
- Tensorflow
- Pytorch

- 飞浆
- Mindspore
- Chainer
- MXNet

## 相关学术会议

- 国际表示学习会议  ICLR
- 神经信息处理系统年会
- 国际机器学习会议 ICML
- 国际人工智能联合会议 IJCAI
- 美国人工智能协会年会 AAAI
- CVPR
- ICCV
- ACL
- EMNLP