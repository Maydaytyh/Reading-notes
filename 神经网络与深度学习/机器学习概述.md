# 基本概念

标记好特征及标签称为一个样本

D维向量表示一个芒果的所有特征构成的向量，称为特征向量

# 机器学习的三个基本要素

## 模型

- 首先要确认输入空间和输出空间
- 输入空间和输出空间一起构成了一个样本空间
- 常见的假设空间分为线性和非线性两种，对应的模型f分为称为线性模型和非线性模型

### 线性模型

线性模式的假设空间为一个参数化的线性函数族
$$
f(x;\theta)=\omega ^ Tx+b
$$
其中$\theta$包含了权重向量$\omega$和偏置$b$

### 非线性模型

$$
f(x;\theta)=\omega ^ T \phi(x)+b
$$

其中，$\phi(x)$为$K$个非线性基函数组成的向量，参数$\theta$包含了权重向量$\omega$和偏置$b$

## 学习准则

模型的好坏可以通过期望风险$R(\theta)$来衡量，

#### 损失函数

##### 0-1损失函数

不连续且导数为0，难以优化

##### 平方损失函数

一般不适用于分类问题

##### 交叉熵损失函数

一般用于分类问题

对于两个概率分布，一般可以用交叉熵来衡量它们的差异，标签的真实分布$\mathcal{y}$和模型预测分布$f(x;\theta)$之间的交叉熵为
$$
\mathcal{L}(y,f(x;\theta))=-y^Tlogf(x;\theta)
=-\sum_{c=1}^{C}y_clogf_c(x;\theta)
$$
因为$y$为one-hot向量，上式也可以写为$-logf_y(x;\theta)$

交叉熵损失函数也就是负对数似然函数

##### Hinge损失函数

二分类问题

#### 风险最小化准则

过拟合

往往是因为训练数据少、噪声以及模型能力强等原因造成的

## 优化算法

- $f(x;\theta)$中的$\theta$称为模型的参数，可以通过优化算法进行学习
- 还有一类参数是用来定义模型结构或优化策略的，这类参数称为超参数，包括，聚类算法中的类别个数，梯度下降法中的步长，正则化项的系数，神经网络的层数

### 梯度下降法

### 提前停止

采用一个验证集，如果在验证集上的错误率不再下降，就停止迭代，这种策略叫做早停

如果没有验证集，则在训练集上划分出一个小比例的子集作为验证集，防止过拟合

### 随机梯度下降算法

从真实数据分布中采集$N$个样本，由他们计算出来的经验风险的梯度来近似期望风险的梯度，即SGD

经过足够次数的迭代，随机梯度下降法也可以收敛到局部最优解

批量梯度下降和随机梯度下降的区别在于，每次迭代的优化目标是对所有样本的平均损失函数还是对单个样本的损失函数

在非凸优化问题中，随机梯度下降更容易逃离局部最优点

### 小批量梯度下降法

随机梯度下降法一个缺点是无法充分利用计算机的并行计算能力

每次迭代时，每次随机选取一小部分训练样本来计算梯度并更新参数，是批量梯度下降和随机梯度下降的折中